{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "colonial-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from tensorflow.keras import backend as kb\n",
    "from resizeimage import resizeimage\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Layer, Dense, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "impressive-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 8)])\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 가상 장치가 설정되어야만 합니다\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "conventional-action",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dandelion', 'sunflowers', 'daisy', 'roses', 'tulips']\n",
      "(70, 5, 180, 180, 3)\n",
      "(30, 5, 180, 180, 3)\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "\n",
    "tmp = list(data_dir.glob('*/'))\n",
    "classes = []\n",
    "for i in tmp:\n",
    "    if os.path.isdir(i):\n",
    "        classes.append(os.path.basename(i))\n",
    "print(classes)\n",
    "\n",
    "Images = []\n",
    "for class_ in classes:\n",
    "    list_ = list(data_dir.glob(class_ + '/*'))\n",
    "    Images.append(list_)\n",
    "    \n",
    "\n",
    "numBatches = 100\n",
    "width = 180\n",
    "height = 180\n",
    "\n",
    "inputs_train = np.empty([1, len(classes), width, height, 3])\n",
    "for i in range(int(numBatches * 0.7)):\n",
    "    \n",
    "    input_ = np.empty([1, width, height, 3])\n",
    "    \n",
    "    for class_ in range(len(classes)):\n",
    "        numClass = len(Images[class_])\n",
    "        choice_idx = np.random.choice(numClass, 1, replace=False)\n",
    "        choice = [Images[class_][j] for j in choice_idx]\n",
    "        \n",
    "        with PIL.Image.open(choice[0]) as img:\n",
    "            im = resizeimage.resize_cover(img, [width, height])\n",
    "            im = np.expand_dims(np.array(im), axis=0)\n",
    "            input_ = np.concatenate((input_, im), axis=0)\n",
    "    input_ = np.delete(input_, 0, axis=0)\n",
    "    \n",
    "    input_ = np.expand_dims(input_, axis=0)\n",
    "    inputs_train = np.concatenate((inputs_train, input_), axis=0)\n",
    "    \n",
    "inputs_train = np.delete(inputs_train, 0, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs_test = np.empty([1, len(classes), width, height, 3])\n",
    "for i in range(int(numBatches * 0.3)):\n",
    "    \n",
    "    input_ = np.empty([1, width, height, 3])\n",
    "    \n",
    "    for class_ in range(len(classes)):\n",
    "        numClass = len(Images[class_])\n",
    "        choice_idx = np.random.choice(numClass, 1, replace=False)\n",
    "        choice = [Images[class_][j] for j in choice_idx]\n",
    "        \n",
    "        with PIL.Image.open(choice[0]) as img:\n",
    "            im = resizeimage.resize_cover(img, [width, height])\n",
    "            im = np.expand_dims(np.array(im), axis=0)\n",
    "            input_ = np.concatenate((input_, im), axis=0)\n",
    "    input_ = np.delete(input_, 0, axis=0)\n",
    "    \n",
    "    input_ = np.expand_dims(input_, axis=0)\n",
    "    inputs_test = np.concatenate((inputs_test, input_), axis=0)\n",
    "    \n",
    "inputs_test = np.delete(inputs_test, 0, axis=0)\n",
    "\n",
    "print(inputs_train.shape)\n",
    "print(inputs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "drawFeatureMap(model, idxBatch = 0, class_ = 1, features = [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "discrete-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawFeatureMap(epoch, model, idxBatch, class_):\n",
    "    \n",
    "    numFeatures = model.numFeatures\n",
    "    inputs = inputs_train[idxBatch,:,:,:,:]\n",
    "    \n",
    "    img_shape = inputs[class_,:,:,0].shape\n",
    "    \n",
    "    width_ratios = [1]\n",
    "    for i in range(math.ceil(numFeatures / 3)):\n",
    "        width_ratios.append(1)\n",
    "    fig, axs = plt.subplots(inputs.shape[-1], math.ceil(numFeatures / inputs.shape[-1]) + 1, gridspec_kw={'width_ratios': width_ratios})\n",
    "    \n",
    "    \n",
    "    for color in range(3):\n",
    "        axs[color, 0].imshow(inputs[class_,:,:,color], cmap='gray', vmin=0, vmax=255)\n",
    "        \n",
    "    outputs = model(inputs)\n",
    "    single_output = outputs[class_,:,:,:]\n",
    "    single_output_abs = tf.math.abs(single_output)\n",
    "    abs_max = tf.math.reduce_max(single_output_abs, axis=-1) + kb.epsilon()\n",
    "    activatedIdx = tf.expand_dims(tf.greater(abs_max, 0.5), axis=-1)\n",
    "    normalized_single_output = single_output / tf.expand_dims(abs_max, axis=-1)\n",
    "    normalized_single_output_activated = tf.where(activatedIdx, normalized_single_output, 0)\n",
    "    \n",
    "    for feature in range(numFeatures):\n",
    "        single_output_FeatureActivated = normalized_single_output_activated[:,:,feature]\n",
    "        pcm = axs[int(feature % 3), int(feature / 3 + 1)].pcolormesh(single_output_FeatureActivated, cmap='RdBu_r')\n",
    "        axs[int(feature % 3), int(feature / 3 + 1)].set_title('Feature : ' + str(feature))\n",
    "        \n",
    "        if feature % 3 == 0:\n",
    "            fig.colorbar(pcm, ax=axs[:,int(feature / 3 + 1)], location='right', shrink=0.6)\n",
    "        \n",
    "    \n",
    "    for i in range(3 - numFeatures%3):\n",
    "        axs[-(i+1), -1].axis('off')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.savefig('savedImgs/' + str(class_) + '/' + str(epoch) + '.jpg')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "amateur-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLoss__(tf.keras.losses.Loss):\n",
    "    \n",
    "    def __init__(self, name=\"distanceLoss\"):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "    def __call__(self, outputs):\n",
    "        #if type_ == 0:\n",
    "        #print(outputs.shape)\n",
    "        featureVectors = tf.zeros((1, model.numFeatures))\n",
    "        for single_output in outputs:\n",
    "            tf.autograph.experimental.set_loop_options(shape_invariants=[(featureVectors, tf.TensorShape([None, model.numFeatures]))])\n",
    "\n",
    "            single_output_activated = tf.where(tf.greater(single_output, 0.5), 1, 0)\n",
    "            \n",
    "            \n",
    "            featureVector = tf.zeros(shape=(1,))\n",
    "            for feature in range(model.numFeatures):\n",
    "                tf.autograph.experimental.set_loop_options(shape_invariants=[(featureVector, tf.TensorShape([None,]))])\n",
    "                \n",
    "                single_output_FeatureActivated = single_output_activated[:,:,feature]\n",
    "                count = tf.expand_dims(tf.cast(tf.math.count_nonzero(single_output_FeatureActivated), float), axis=0)\n",
    "                featureVector = tf.concat([featureVector, count], axis=0)\n",
    "            featureVector = featureVector[1:]\n",
    "            #print(featureVector.shape)\n",
    "            \n",
    "            featureVector = tf.expand_dims(featureVector, axis=0)\n",
    "            featureVectors = tf.concat([featureVectors, featureVector], axis=0)\n",
    "            \n",
    "        \n",
    "        featureVectors = featureVectors[1:,:]\n",
    "        \n",
    "        #print(featureVectors)\n",
    "        max_ = tf.expand_dims(tf.reduce_max(featureVectors, axis=1), axis=-1)\n",
    "        min_ = tf.expand_dims(tf.reduce_min(featureVectors, axis=1), axis=-1)\n",
    "        #print(max_.shape)\n",
    "        \n",
    "        featureVectors = tf.subtract(featureVectors, min_) / tf.subtract(max_, min_)\n",
    "        #print(featureVectors)\n",
    "        \n",
    "        #print('!!')\n",
    "        #print(featureVectors)\n",
    "        featureVector_mean = tf.math.reduce_mean(featureVectors, axis=0)\n",
    "        \n",
    "        #print(featureVector_mean)\n",
    "        tmp = tf.math.subtract(featureVectors, tf.expand_dims(featureVector_mean, axis=0))\n",
    "        distance2mean = tf.norm(tmp, axis=1)\n",
    "        distanceSum = tf.math.reduce_sum(distance2mean)\n",
    "        #print(distance2mean)\n",
    "        #print(distanceSum)\n",
    "        Loss = tf.math.exp(-distanceSum)\n",
    "        \n",
    "        return distanceSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "statewide-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLoss(tf.keras.losses.Loss):\n",
    "    \n",
    "    def __init__(self, name=\"distanceLoss\"):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "    def __call__(self, outputs):\n",
    "        #if type_ == 0:\n",
    "        #print(outputs.shape)\n",
    "        featureVectors = tf.zeros((1, model.numFeatures))\n",
    "        for single_output in outputs:\n",
    "            tf.autograph.experimental.set_loop_options(shape_invariants=[(featureVectors, tf.TensorShape([None, model.numFeatures]))])\n",
    "            single_output_abs = tf.math.abs(single_output)\n",
    "            abs_max = tf.math.reduce_max(single_output_abs, axis=-1) + kb.epsilon()\n",
    "            activatedIdx = tf.expand_dims(tf.greater(abs_max, 0.5), axis=-1)\n",
    "            normalized_single_output = single_output / tf.expand_dims(abs_max, axis=-1)\n",
    "            normalized_single_output_activated = tf.where(activatedIdx, normalized_single_output, 0)\n",
    "            \n",
    "            featureVector = tf.zeros(shape=(1,))\n",
    "            for feature in range(model.numFeatures):\n",
    "                tf.autograph.experimental.set_loop_options(shape_invariants=[(featureVector, tf.TensorShape([None,]))])\n",
    "                \n",
    "                single_output_FeatureActivated = normalized_single_output_activated[:,:,feature]\n",
    "                #print(single_output_FeatureActivated.shape)\n",
    "                sum_ = tf.expand_dims(tf.math.reduce_sum(single_output_FeatureActivated), axis=0)\n",
    "                \n",
    "                featureVector = tf.concat([featureVector, sum_], axis=0)\n",
    "            featureVector = featureVector[1:]\n",
    "            #print(featureVector.shape)\n",
    "            \n",
    "            featureVector = tf.expand_dims(featureVector, axis=0)\n",
    "            featureVectors = tf.concat([featureVectors, featureVector], axis=0)\n",
    "            \n",
    "        featureVectors = featureVectors[1:,:]\n",
    "        #print(featureVectors)\n",
    "        featureVectors_abs = tf.math.abs(featureVectors)\n",
    "        abs_max = tf.math.reduce_max(featureVectors_abs, axis=-1) +  + kb.epsilon()\n",
    "        #print(abs_max.shape)\n",
    "        featureVectors = featureVectors / tf.expand_dims(abs_max, axis=-1)\n",
    "        \n",
    "        \n",
    "        #print(featureVectors)\n",
    "        \n",
    "        \n",
    "        featureVector_mean = tf.math.reduce_mean(featureVectors, axis=0)\n",
    "        \n",
    "        #print(featureVector_mean)\n",
    "        #print('!!')\n",
    "        \n",
    "        tmp = tf.math.subtract(featureVectors, tf.expand_dims(featureVector_mean, axis=0))\n",
    "        distance2mean = tf.norm(tmp, axis=1)\n",
    "        distanceSum = tf.math.reduce_sum(distance2mean)\n",
    "        Loss = tf.math.exp(-distanceSum)\n",
    "            \n",
    "        \n",
    "        return Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "persistent-cookie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-1.3828938   0.43966636  0.2457787 ]\n",
      "  [ 0.63606316 -0.6373113   0.8395073 ]]\n",
      "\n",
      " [[ 0.7769656  -0.44148183  0.97496396]\n",
      "  [ 1.1209984   0.8038084  -0.6012371 ]]], shape=(2, 2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[False False]\n",
      " [False  True]], shape=(2, 2), dtype=bool)\n",
      "tf.Tensor(\n",
      "[[[ 0.         0.         0.       ]\n",
      "  [ 0.         0.         0.       ]]\n",
      "\n",
      " [[ 0.         0.         0.       ]\n",
      "  [ 1.1209984  0.8038084 -0.6012371]]], shape=(2, 2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tmp1 = tf.random.normal(shape=(2, 2, 3))\n",
    "print(tmp1)\n",
    "tmp2 = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "tmp2 = tf.greater(tmp2, 0.5)\n",
    "print(tmp2)\n",
    "tmp3 = tf.expand_dims(tmp2, axis=-1)\n",
    "#print(tmp3)\n",
    "tmp4 = tf.where(tmp3, tmp1, 0)\n",
    "print(tmp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "robust-frank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.19576989  1.765268   -0.9999262 ]\n",
      "  [-0.62681615 -0.44974372 -0.31097123]]\n",
      "\n",
      " [[ 0.2895611  -1.4253116  -0.8290877 ]\n",
      "  [ 0.6379421   0.19654004  0.6517966 ]]], shape=(2, 2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[0.19576989 1.765268   0.9999262 ]\n",
      "  [0.62681615 0.44974372 0.31097123]]\n",
      "\n",
      " [[0.2895611  1.4253116  0.8290877 ]\n",
      "  [0.6379421  0.19654004 0.6517966 ]]], shape=(2, 2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.765268   0.62681615]\n",
      " [1.4253116  0.6517966 ]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[ 0.11090095  1.         -0.56644446]\n",
      "  [-1.         -0.717505   -0.49611235]]\n",
      "\n",
      " [[ 0.20315635 -1.         -0.5816887 ]\n",
      "  [ 0.97874415  0.30153587  1.        ]]], shape=(2, 2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tmp1 = tf.random.normal(shape=(2, 2, 3))\n",
    "print(tmp1)\n",
    "tmp2 = tf.math.abs(tmp1)\n",
    "print(tmp2)\n",
    "tmp3 = tf.math.reduce_max(tmp2, axis=-1)\n",
    "print(tmp3)\n",
    "tmp4 = tmp1 / tf.expand_dims(tmp3, axis=-1)\n",
    "print(tmp4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "increasing-statement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tmp = tf.zeros(shape=(1,))\n",
    "print(tmp)\n",
    "\n",
    "loss1 = customLoss()\n",
    "\n",
    "for i in range(inputs_train.shape[0]):\n",
    "    inputs = inputs_train[i,:,:,:,:]\n",
    "    #print(inputs.shape)\n",
    "    outputs = model(inputs)\n",
    "    l1 = loss1(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "small-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self, numFeatures=3):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.numFeatures = numFeatures\n",
    "        self.input_shape_ = None\n",
    "        self.custom_layers = []\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.input_shape_ = input_shape[1:]\n",
    "        \n",
    "        self.custom_layers.append(Conv2D(self.numFeatures, 3, padding=\"same\", input_shape=self.input_shape_))\n",
    "        self.pooling_ = MaxPool2D()\n",
    "            \n",
    "    def addLayer(self, numFeatures):\n",
    "        self.numFeatures = numFeatures\n",
    "        \"\"\"\n",
    "        shape = tf.convert_to_tensor(self.input_shape_).numpy()\n",
    "        for i in range(len(self.custom_layers)):\n",
    "            shape[0] = shape[0] - 2\n",
    "            shape[1] = shape[1] - 2\n",
    "         \n",
    "        self.custom_layers.append(Conv2D(self.numFeatures, 3, input_shape=shape))\n",
    "        \"\"\"\n",
    "        self.custom_layers.append(Conv2D(self.numFeatures, 3, padding=\"same\", input_shape=self.input_shape_))\n",
    "        self.custom_layers[-1].build(self.input_shape_)\n",
    "        \n",
    "            \n",
    "    def addNode(self, numFeatures):\n",
    "        weights = self.custom_layers[-1].get_weights()\n",
    "        \n",
    "        #print(weights[0].shape)\n",
    "        #print(weights[1].shape)\n",
    "        \n",
    "        shape = list(weights[0].shape[:-1])\n",
    "        shape.append(numFeatures)\n",
    "        #print(shape)\n",
    "        \n",
    "        #shape = np.expand_dims(shape, axis=-1)\n",
    "        tmp = np.random.normal(size=shape)\n",
    "        weights[0] = np.concatenate((weights[0], tmp), axis=-1)\n",
    "        #print(weights[0].shape)\n",
    "        \n",
    "        tmp = np.random.normal(size = (numFeatures,))\n",
    "        weights[1] = np.concatenate((weights[1], tmp))\n",
    "        \n",
    "        \"\"\"\n",
    "        #print(weights[1].shape)\n",
    "        shape = tf.convert_to_tensor(self.input_shape_).numpy()\n",
    "        for i in range(len(self.custom_layers) - 1):\n",
    "            shape[0] = shape[0] - 2\n",
    "            shape[1] = shape[1] - 2\n",
    "        \"\"\"\n",
    "        self.numFeatures += numFeatures\n",
    "        self.custom_layers[-1] = Conv2D(self.numFeatures, 3, input_shape=self.input_shape_)\n",
    "        self.custom_layers[-1].build(self.input_shape_)\n",
    "        self.custom_layers[-1].set_weights(weights)\n",
    "        \n",
    "        #self.custom_layers[-1] = Conv2D(self.numFeatures, 3, input_shape=shape)\n",
    "        #self.custom_layers[-1].build(shape)\n",
    "        #self.custom_layers[-1].set_weights(weights)\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        for idx_layer, layer_ in enumerate(self.custom_layers):\n",
    "            x = tf.math.sin(layer_(x))\n",
    "            #x = self.pooling_(x)\n",
    "        return x\n",
    "\n",
    "model = MyModel(numFeatures=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "extensive-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "loss1 = customLoss()\n",
    "L1_train = tf.keras.metrics.Mean()\n",
    "#L2_train = tf.keras.metrics.Mean()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        l1 = loss1(outputs)\n",
    "        Loss = l1\n",
    "        \n",
    "    gradients = tape.gradient(Loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    L1_train(l1)\n",
    "    \n",
    "L1_valid = tf.keras.metrics.Mean()\n",
    "#L2_valid = tf.keras.metrics.Mean()\n",
    "@tf.function\n",
    "def test_step(inputs):\n",
    "    outputs = model(inputs)\n",
    "    l1 = loss1(outputs)\n",
    "    L1_valid(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "returning-sleeping",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-203c539616b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'에포크: {}, L1_train: {:.4f}, L1_vaid: {:.4f}, 걸린 시간: {:.3f}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 20000\n",
    "patience = 300\n",
    "stopped_epoch = 0\n",
    "best_weights = None\n",
    "best = np.Inf\n",
    "wait = 0\n",
    "\n",
    "history_l1_train = []\n",
    "#history_l2_train = []\n",
    "history_l1_valid = []\n",
    "#history_l2_valid = []\n",
    "\n",
    "\n",
    "#model.addNode(2)\n",
    "#model.addLayer(20)\n",
    "start_time = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    L1_train.reset_states()\n",
    "    #L2_train.reset_states()\n",
    "    for i in range(inputs_train.shape[0]):\n",
    "        inputs = inputs_train[i,:,:,:,:]\n",
    "        train_step(inputs)\n",
    "    \n",
    "    L1_valid.reset_states()\n",
    "    #L2_valid.reset_states()\n",
    "    for i in range(inputs_test.shape[0]):\n",
    "        inputs = inputs_test[i,:,:,:,:]\n",
    "        test_step(inputs)\n",
    "    \n",
    "    template = '에포크: {}, L1_train: {:.4f}, L1_vaid: {:.4f}, 걸린 시간: {:.3f}'\n",
    "    print (template.format(epoch+1,\n",
    "                         L1_train.result(),\n",
    "                         L1_valid.result(),\n",
    "                           (time.time() - start_time)))\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        for class_ in range(5):\n",
    "            drawFeatureMap(epoch = epoch, model = model, idxBatch = 0, class_ = class_)\n",
    "            \n",
    "\n",
    "    \"\"\"\n",
    "    template = '에포크: {}, L1_train: {:.4f}, L2_train: {:.4f}, L1_vaid: {:.4f}, L2_valid: {:.4f}, 걸린 시간: {:.3f}'\n",
    "    print (template.format(epoch+1,\n",
    "                         L1_train.result(),\n",
    "                           L2_train.result(),\n",
    "                         L1_valid.result(),\n",
    "                           L2_valid.result(),\n",
    "                           (time.time() - start_time)))\n",
    "    \"\"\"\n",
    "    \n",
    "    history_l1_train.append(L1_train.result())\n",
    "    #history_l2_train.append(L2_train.result())\n",
    "    \n",
    "    history_l1_valid.append(L1_valid.result())\n",
    "    #history_l2_valid.append(L2_valid.result())\n",
    "    \n",
    "    \"\"\"\n",
    "    if np.less(float(L1_valid.result() + L2_valid.result()), best):\n",
    "        best = float(L1_valid.result() + L2_valid.result())\n",
    "        best_weights = model.get_weights()\n",
    "        wait = 0\n",
    "    \"\"\"\n",
    "    if np.less(float(L1_valid.result()), best):\n",
    "        best = float(L1_valid.result())\n",
    "        best_weights = model.get_weights()\n",
    "        wait = 0\n",
    "        \n",
    "    else:\n",
    "        wait +=1\n",
    "        if wait >= patience:\n",
    "            model.set_weights(best_weights)\n",
    "            stopped_epoch = epoch\n",
    "            print('Early Stopped !')\n",
    "            break\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(1, figsize=(18,20))\n",
    "\n",
    "ax.set_title('L1')\n",
    "x = np.arange(len(history_l1_train))\n",
    "ax.plot(x, history_l1_train, 'r-', label = 'trainLoss')\n",
    "ax.plot(x, history_l1_valid, 'b-', label = 'validLoss')\n",
    "ax.legend()\n",
    "\n",
    "\"\"\"\n",
    "ax[1].set_title('L2')\n",
    "ax[1].plot(x, history_l2_train, 'r-', label = 'trainLoss')\n",
    "ax[1].plot(x, history_l2_valid, 'b-', label = 'validLoss')\n",
    "ax[1].legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-rebate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
