{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "russian-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as kb\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "peaceful-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions can be used to convert a value to a type compatible\n",
    "# with tf.train.Example.\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "painted-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image_name': tf.io.FixedLenFeature([], tf.string),\n",
    "    'target': tf.io.FixedLenFeature([], tf.string)\n",
    "}\n",
    "def _parse_image_function(example_proto):\n",
    "  # Parse the input tf.train.Example proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "def features_preprocess(features):\n",
    "    \n",
    "    # Image\n",
    "    image_raw = features['image']\n",
    "    image_ = tf.io.decode_image(image_raw)\n",
    "    image = tf.cast(image_ , tf.float32) * (1. / 255)\n",
    "    \n",
    "    # Image name\n",
    "    image_name_raw = features['image_name'].numpy().decode()\n",
    "    image_name = image_name_raw.split('_')[0]\n",
    "    image_color = image_name_raw.split('_')[1]\n",
    "    \n",
    "    # Image Label\n",
    "    target_raw = features['target'].numpy().decode()\n",
    "    target_list = target_raw.split('|')\n",
    "\n",
    "    return image, image_name, image_color, target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-lebanon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial_num: 9, 에포크: 262, 손실: 0.4879, 정확도: 80.196, 테스트 손실: 0.4875, 테스트 정확도: 80.047, 소요시간: 3.325\n"
     ]
    }
   ],
   "source": [
    "test_files = os.listdir('./test_tfrecords')\n",
    "test_files.sort()\n",
    "\n",
    "train_files = os.listdir('./train_tfrecords')\n",
    "train_files.sort()\n",
    "\n",
    "dataset = None\n",
    "dataset_batched = None\n",
    "trainDataset = None\n",
    "validDataset = None\n",
    "\n",
    "trial = 200\n",
    "\n",
    "for trial_ in range(trial):\n",
    "    selected = np.random.choice(len(train_files), 1, replace=False)\n",
    "    \n",
    "    count_positive = 0\n",
    "    inputs_positive = np.empty([1, 512, 512, 4])\n",
    "    outputs_positive = np.empty([1, 1])\n",
    "    \n",
    "    count_negative = 0\n",
    "    inputs_negative = np.empty([1, 512, 512, 4])\n",
    "    outputs_negative = np.empty([1, 1])\n",
    "    \n",
    "    ratio_positive = np.random.uniform(low=0.3, high=0.7)\n",
    "    \n",
    "    buffer = {}\n",
    "    current_file = ''\n",
    "    \n",
    "    for idx in selected:\n",
    "        dir_ = os.path.join('train_tfrecords', train_files[idx])\n",
    "        \n",
    "        raw_dataset = tf.data.TFRecordDataset(dir_)\n",
    "        parsed_dataset = raw_dataset.map(_parse_image_function)\n",
    "        \n",
    "        for features in parsed_dataset:\n",
    "            image, image_name, image_color, target_list = features_preprocess(features)\n",
    "\n",
    "            if current_file != image_name:\n",
    "                buffer = {}\n",
    "                current_file = image_name\n",
    "\n",
    "            buffer[image_color] = image\n",
    "\n",
    "            # 하나의 이미지에 대한 4개의 색깔 이미지가 다 보인 시점\n",
    "            if len(buffer.keys()) == 4:\n",
    "\n",
    "                if '0' in target_list:\n",
    "                    \n",
    "                    count_positive += 1\n",
    "                    input_ = tf.expand_dims(tf.concat([buffer['blue'], buffer['yellow'], buffer['green'], buffer['red'] ], 2), 0)\n",
    "                    input_ = input_[:, 256:768, 256:768, :]\n",
    "                    output_ = tf.constant([[1]])\n",
    "\n",
    "                    inputs_positive = np.concatenate((inputs_positive, input_), axis=0)\n",
    "                    outputs_positive = np.concatenate((outputs_positive, output_), axis=0)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    count_negative += 1\n",
    "                    input_ = tf.expand_dims(tf.concat([buffer['blue'], buffer['yellow'], buffer['green'], buffer['red'] ], 2), 0)\n",
    "                    input_ = input_[:, 256:768, 256:768, :]\n",
    "                    output_ = tf.constant([[0]])\n",
    "\n",
    "                    inputs_negative = np.concatenate((inputs_negative, input_), axis=0)\n",
    "                    outputs_negative = np.concatenate((outputs_negative, output_), axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    inputs_positive = np.delete(inputs_positive, 0, axis=0)\n",
    "    outputs_positive = np.delete(outputs_positive, 0, axis=0)\n",
    "    \n",
    "    inputs_negative = np.delete(inputs_negative, 0, axis=0)\n",
    "    outputs_negative = np.delete(outputs_negative, 0, axis=0)\n",
    "    \n",
    "    if count_positive / ratio_positive * (1 - ratio_positive) < count_negative:\n",
    "        choise_ = np.random.choice(count_negative, int(count_negative * (1 - ratio_positive)), replace=False)\n",
    "        inputs_negative = inputs_negative[choise_,:,:,:]\n",
    "        outputs_negative = outputs_negative[choise_,:]\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        choise_ = np.random.choice(count_positive, int(count_positive * ratio_positive), replace=False)\n",
    "        inputs_positive = inputs_positive[choise_,:,:,:]\n",
    "        outputs_positive = outputs_positive[choise_,:]\n",
    "        \n",
    "        \n",
    "        \n",
    "    inputs = np.concatenate((inputs_positive, inputs_negative), axis=0)\n",
    "    outputs = np.concatenate((outputs_positive, outputs_negative), axis=0)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs)).shuffle(inputs.shape[0], reshuffle_each_iteration=True)\n",
    "    dataset_batched = dataset.batch(16, drop_remainder=True)\n",
    "    numberOfData = dataset_batched.cardinality().numpy()\n",
    "    trainDataset = dataset_batched.take(int(numberOfData * 0.7))\n",
    "    validDataset = dataset_batched.skip(int(numberOfData * 0.7))\n",
    "    \n",
    "    \n",
    "    train_session(trial_+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "scenic-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self, list_units):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, padding='same', activation='swish')\n",
    "        self.pool1 = MaxPooling2D()\n",
    "        \n",
    "        self.conv2 = Conv2D(32, 3, padding='same', activation='swish')\n",
    "        self.pool2 = MaxPooling2D()\n",
    "        \n",
    "        self.conv3 = Conv2D(32, 3, padding='same', activation='swish')\n",
    "        self.pool3 = MaxPooling2D()\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        self.layers_custom = self.create_layers(list_units)\n",
    "        self.lastLayer = Dense(1, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(0.001))\n",
    "        \n",
    "    def create_layers(self, list_units):\n",
    "        layers = []\n",
    "        for units in list_units:\n",
    "            layers.append(Dense(units, activation='swish', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "        return layers\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        for layer in self.layers_custom:\n",
    "            x = layer(x)\n",
    "        x = self.lastLayer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = MyModel([7, 7])\n",
    "#model.load_weights('./kaggle_models/class0_77_file1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dominican-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean()\n",
    "train_acc = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "        train_loss(loss)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_acc.update_state(labels, predictions)\n",
    "\n",
    "    \n",
    "    \n",
    "valid_loss = tf.keras.metrics.Mean()\n",
    "valid_acc = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    valid_loss(t_loss)\n",
    "    valid_acc.update_state(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "subtle-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_session(trial_number):\n",
    "    \n",
    "    global dataset\n",
    "    global dataset_batched\n",
    "    global trainDataset\n",
    "    global validDataset\n",
    "    \n",
    "    EPOCHS = 1000\n",
    "    patience = 100\n",
    "    stopped_epoch = 0\n",
    "    best_weights = None\n",
    "    best = np.Inf\n",
    "    wait = 0\n",
    "\n",
    "\n",
    "    history_trainLoss = []\n",
    "    history_validLoss = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        clear_output(wait=True)\n",
    "        start_time = time.time()\n",
    "\n",
    "        for images, labels in trainDataset:\n",
    "            train_step(images, tf.expand_dims(labels, axis=-1))\n",
    "        for images, labels in validDataset:\n",
    "            test_step(images, tf.expand_dims(labels, axis=-1))\n",
    "\n",
    "        template = 'Trial_num: {}, 에포크: {}, 손실: {:.4f}, 정확도: {:.3f}, 테스트 손실: {:.4f}, 테스트 정확도: {:.3f}, 소요시간: {:.3f}'\n",
    "        print (template.format(trial_number,\n",
    "                               epoch+1,\n",
    "                             train_loss.result(),\n",
    "                             train_acc.result()*100,\n",
    "                             valid_loss.result(),\n",
    "                             valid_acc.result()*100,\n",
    "                               time.time()-start_time))\n",
    "\n",
    "        history_trainLoss.append(train_loss.result())\n",
    "        history_validLoss.append(valid_loss.result())\n",
    "\n",
    "        if np.less(float(valid_loss.result()), best):\n",
    "            best = float(valid_loss.result())\n",
    "            best_weights = model.get_weights()\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait +=1\n",
    "            if wait >= patience:\n",
    "                model.set_weights(best_weights)\n",
    "                stopped_epoch = epoch\n",
    "                print('Early Stopped !')\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "    fig= plt.figure(figsize=(18,20))\n",
    "    plt.title('Loss')\n",
    "    x = np.arange(len(history_trainLoss))\n",
    "    plt.plot(x, history_trainLoss, 'r-', label = 'history_trainLoss')\n",
    "    plt.plot(x, history_validLoss, 'b-', label = 'history_validLoss')\n",
    "    plt.legend()\n",
    "\n",
    "    model.save_weights('./kaggle_models/trial_' + str(trial_number))\n",
    "    \n",
    "    dataset = None\n",
    "    dataset_batched = None\n",
    "    trainDataset = None\n",
    "    validDataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "medium-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial = 100\n",
    "\n",
    "for i in range(trial):\n",
    "    np.random.choice(len(train_files), 2, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "advance-execution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "tmp1 = tf.constant(1, shape=(3, 2, 1))\n",
    "tmp2 = tf.constant(1, shape=(3, 2, 1))\n",
    "print(tf.concat([tmp1, tmp2], 2).shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "operational-integrity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31551865344251245"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-brass",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
