{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from tensorflow.keras import backend as kb\n",
    "from resizeimage import resizeimage\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Layer, Dense, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 8)])\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 가상 장치가 설정되어야만 합니다\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "\n",
    "tmp = list(data_dir.glob('*/'))\n",
    "classes = []\n",
    "for i in tmp:\n",
    "    if os.path.isdir(i):\n",
    "        classes.append(os.path.basename(i))\n",
    "print(classes)\n",
    "\n",
    "Images = []\n",
    "for class_ in classes:\n",
    "    list_ = list(data_dir.glob(class_ + '/*'))\n",
    "    Images.append(list_)\n",
    "    \n",
    "\n",
    "numBatches = 100\n",
    "numImagesOfClassesForBatch = 4\n",
    "width = 180\n",
    "height = 180\n",
    "\n",
    "inputs_train = np.empty([1, len(classes), numImagesOfClassesForBatch, width, height, 3])\n",
    "for i in range(int(numBatches * 0.7)):\n",
    "    \n",
    "    input_ = np.empty([1, numImagesOfClassesForBatch, width, height, 3])\n",
    "    \n",
    "    for class_ in range(len(classes)):\n",
    "        numClass = len(Images[class_])\n",
    "        choice_idx = np.random.choice(numClass, numImagesOfClassesForBatch, replace=False)\n",
    "        choice = [Images[class_][j] for j in choice_idx]\n",
    "        \n",
    "        \n",
    "        buffer = np.empty([1, width, height, 3])\n",
    "        \n",
    "        for path in choice:\n",
    "            with PIL.Image.open(path) as img:\n",
    "                im = resizeimage.resize_cover(img, [width, height])\n",
    "                im = np.expand_dims(np.array(im), axis=0)\n",
    "                buffer = np.concatenate((buffer, im), axis=0)\n",
    "                \n",
    "                #print(im.shape)\n",
    "                #plt.imshow(im)\n",
    "                \n",
    "        buffer = np.delete(buffer, 0, axis=0)\n",
    "        buffer = np.expand_dims(buffer, axis=0)\n",
    "        \n",
    "        input_ = np.concatenate((input_, buffer), axis=0)\n",
    "        \n",
    "        \n",
    "        #print(buffer.shape)\n",
    "        \n",
    "        \n",
    "    input_ = np.delete(input_, 0, axis=0)\n",
    "    input_ = np.expand_dims(input_, axis=0)\n",
    "    \n",
    "    inputs_train = np.concatenate((inputs_train, input_), axis=0)\n",
    "inputs_train = np.delete(inputs_train, 0, axis=0)\n",
    "\n",
    "\n",
    "inputs_test = np.empty([1, len(classes), numImagesOfClassesForBatch, width, height, 3])\n",
    "for i in range(int(numBatches * 0.3)):\n",
    "    \n",
    "    input_ = np.empty([1, numImagesOfClassesForBatch, width, height, 3])\n",
    "    \n",
    "    for class_ in range(len(classes)):\n",
    "        numClass = len(Images[class_])\n",
    "        choice_idx = np.random.choice(numClass, numImagesOfClassesForBatch, replace=False)\n",
    "        choice = [Images[class_][j] for j in choice_idx]\n",
    "        \n",
    "        \n",
    "        buffer = np.empty([1, width, height, 3])\n",
    "        \n",
    "        for path in choice:\n",
    "            with PIL.Image.open(path) as img:\n",
    "                im = resizeimage.resize_cover(img, [width, height])\n",
    "                im = np.expand_dims(np.array(im), axis=0)\n",
    "                buffer = np.concatenate((buffer, im), axis=0)\n",
    "                \n",
    "                #print(im.shape)\n",
    "                #plt.imshow(im)\n",
    "                \n",
    "        buffer = np.delete(buffer, 0, axis=0)\n",
    "        buffer = np.expand_dims(buffer, axis=0)\n",
    "        \n",
    "        input_ = np.concatenate((input_, buffer), axis=0)\n",
    "        \n",
    "        \n",
    "        #print(buffer.shape)\n",
    "        \n",
    "        \n",
    "    input_ = np.delete(input_, 0, axis=0)\n",
    "    input_ = np.expand_dims(input_, axis=0)\n",
    "    \n",
    "    inputs_test = np.concatenate((inputs_test, input_), axis=0)\n",
    "inputs_test = np.delete(inputs_test, 0, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(inputs_train.shape)\n",
    "print(inputs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized(x):\n",
    "    min_ = tf.reduce_min(x)\n",
    "    max_ = tf.reduce_max(x)\n",
    "    \n",
    "    y = (tf.subtract(x, min_)/ tf.subtract(max_, min_))\n",
    "    return y\n",
    "\n",
    "def distanceLoss(outputs):\n",
    "    total_mean = None\n",
    "    class_means = []\n",
    "    \n",
    "    for i in range(outputs.shape[0]):\n",
    "        output_class = outputs[i,:,:,:,:]\n",
    "        output_class_mean = tf.math.reduce_mean(output_class, axis=0)\n",
    "        class_means.append(output_class_mean)\n",
    "        \n",
    "        output_class_mean = tf.expand_dims(output_class_mean, axis=0)\n",
    "        \n",
    "        if total_mean is None:\n",
    "            total_mean = output_class_mean\n",
    "        else:\n",
    "            total_mean = tf.concat([total_mean, output_class_mean], axis=0)\n",
    "    \n",
    "    total_mean = tf.math.reduce_mean(total_mean, axis=0)\n",
    "    #total_mean = normalized(total_mean)\n",
    "    \n",
    "    distance_sum = tf.constant(0, dtype=tf.float32)\n",
    "    for i in class_means:\n",
    "        #output_class_mean = normalized(i)\n",
    "        distance = tf.reduce_sum(tf.abs(tf.subtract(total_mean, i)), [0, 1, 2])\n",
    "        #distance = tf.reduce_sum(tf.math.multiply(total_mean, output_class_mean), [0, 1, 2])\n",
    "        distance_sum += distance\n",
    "    \n",
    "    #distance_mean /= outputs.shape[0]\n",
    "    Loss = tf.math.exp(-tf.norm(distance_sum))\n",
    "    #Loss = 1 / (tf.norm(distance_mean) + kb.epsilon())\n",
    "    return Loss\n",
    "\n",
    "\n",
    "def distanceLoss2(outputs):\n",
    "    \n",
    "    distance_means = tf.constant(0, dtype=tf.float32)\n",
    "    \n",
    "    for i in range(outputs.shape[0]):\n",
    "        output_class = outputs[i,:,:,:,:]\n",
    "        output_class_mean = tf.math.reduce_mean(output_class, axis=0)\n",
    "        #output_class_mean = normalized(output_class_mean)\n",
    "        \n",
    "        distance_mean = tf.constant(0, dtype=tf.float32)\n",
    "        for j in range(output_class.shape[0]):\n",
    "            single_output = output_class[j,:,:,:]\n",
    "            #single_output = normalized(single_output)\n",
    "            #distance = tf.reduce_sum(tf.math.multiply(output_class_mean, single_output), [0, 1, 2])\n",
    "            distance = tf.reduce_sum(tf.abs(tf.subtract(output_class_mean, single_output)), [0, 1, 2])\n",
    "            distance_mean += distance\n",
    "        distance_mean /= output_class.shape[0]\n",
    "        distance_means += distance_mean\n",
    "    \n",
    "    distance_means /= outputs.shape[0]\n",
    "    Loss = distance_means\n",
    "    #print(Loss)\n",
    "    return Loss\n",
    "            \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self, numFeatures=3):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.numFeatures = numFeatures\n",
    "        self.input_shape_ = None\n",
    "        self.custom_layers = []\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.input_shape_ = input_shape[1:]\n",
    "        \n",
    "        self.custom_layers.append(Conv2D(self.numFeatures, 3, input_shape=self.input_shape_))\n",
    "        self.pooling_ = MaxPool2D()\n",
    "            \n",
    "    def addLayer(self, numFeatures):\n",
    "        self.numFeatures = numFeatures\n",
    "        shape = tf.convert_to_tensor(self.input_shape_).numpy()\n",
    "        for i in range(len(self.custom_layers)):\n",
    "            shape[0] = shape[0] - 2\n",
    "            shape[1] = shape[1] - 2\n",
    "            \n",
    "        self.custom_layers.append(Conv2D(self.numFeatures, 3, input_shape=shape))\n",
    "        self.custom_layers[-1].build(shape)\n",
    "        \n",
    "            \n",
    "    def addNode(self, numFeatures):\n",
    "        weights = self.custom_layers[-1].get_weights()\n",
    "        \n",
    "        #print(weights[0].shape)\n",
    "        #print(weights[1].shape)\n",
    "        \n",
    "        shape = list(weights[0].shape[:-1])\n",
    "        shape.append(numFeatures)\n",
    "        #print(shape)\n",
    "        \n",
    "        #shape = np.expand_dims(shape, axis=-1)\n",
    "        tmp = np.random.normal(size=shape)\n",
    "        weights[0] = np.concatenate((weights[0], tmp), axis=-1)\n",
    "        #print(weights[0].shape)\n",
    "        \n",
    "        tmp = np.random.normal(size = (numFeatures,))\n",
    "        weights[1] = np.concatenate((weights[1], tmp))\n",
    "        \n",
    "        #print(weights[1].shape)\n",
    "        shape = tf.convert_to_tensor(self.input_shape_).numpy()\n",
    "        for i in range(len(self.custom_layers) - 1):\n",
    "            shape[0] = shape[0] - 2\n",
    "            shape[1] = shape[1] - 2\n",
    "            \n",
    "        self.numFeatures += numFeatures\n",
    "        self.custom_layers[-1] = Conv2D(self.numFeatures, 3, input_shape=shape)\n",
    "        self.custom_layers[-1].build(shape)\n",
    "        self.custom_layers[-1].set_weights(weights)\n",
    "\n",
    "    def call(self, x):\n",
    "        #results = {}\n",
    "        \n",
    "        for idx_layer, layer_ in enumerate(self.custom_layers):\n",
    "            x = tf.math.sin(layer_(x))\n",
    "            x = self.pooling_(x)\n",
    "        \n",
    "        \"\"\"\n",
    "        shape = list(x.shape[1:])\n",
    "        shape.insert(0, 1)\n",
    "        answer = tf.random.normal(shape=shape)\n",
    "        for i in range(x.shape[0]):\n",
    "            single_out = x[i,:,:,:]\n",
    "            single_out_normalized = normalized(single_out)\n",
    "            single_out_normalized = tf.expand_dims(single_out_normalized, axis=0)\n",
    "            \n",
    "            answer = tf.concat([answer, single_out_normalized], axis=0)\n",
    "        answer = answer[1:,:,:,:]\n",
    "        \"\"\"\n",
    "        return x\n",
    "\n",
    "model = MyModel(numFeatures=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tf.random.normal(shape=(4, 180, 180, 3))\n",
    "answer = model(tmp)\n",
    "\n",
    "print(answer.shape)\n",
    "\"\"\"\n",
    "shape = list(answer.shape[1:])\n",
    "shape.insert(0, 1)\n",
    "tmp = tf.random.normal(shape=shape)\n",
    "print(tmp.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.002)\n",
    "L1_train = tf.keras.metrics.Mean()\n",
    "L2_train = tf.keras.metrics.Mean()\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = None\n",
    "        for i in range(input_.shape[0]):\n",
    "            input_oneClass = input_[i,:,:,:,:]\n",
    "            output_oneClass = model(input_oneClass)\n",
    "            output_oneClass = tf.expand_dims(output_oneClass, axis=0)\n",
    "            \n",
    "            if output == None:\n",
    "                output = output_oneClass\n",
    "            else:\n",
    "                output = tf.concat([output, output_oneClass], axis=0)\n",
    "        \n",
    "        l1 = distanceLoss(output)\n",
    "        l2 = distanceLoss2(output)\n",
    "        Loss = l1 + l2\n",
    "        #Loss = distanceLoss2(output)\n",
    "        \n",
    "    gradients = tape.gradient(Loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    L1_train(l1)\n",
    "    L2_train(l2)\n",
    "    \n",
    "L1_valid = tf.keras.metrics.Mean()\n",
    "L2_valid = tf.keras.metrics.Mean()\n",
    "@tf.function\n",
    "def test_step(input_):\n",
    "    output = None\n",
    "    for i in range(input_.shape[0]):\n",
    "        input_oneClass = input_[i,:,:,:,:]\n",
    "        output_oneClass = model(input_oneClass)\n",
    "        output_oneClass = tf.expand_dims(output_oneClass, axis=0)\n",
    "        \n",
    "        if output == None:\n",
    "            output = output_oneClass\n",
    "        else:\n",
    "            output = tf.concat([output, output_oneClass], axis=0)\n",
    "            \n",
    "    l1 = distanceLoss(output)\n",
    "    l2 = distanceLoss2(output)\n",
    "    L1_valid(l1)\n",
    "    L2_valid(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20000\n",
    "patience = 100\n",
    "stopped_epoch = 0\n",
    "best_weights = None\n",
    "best = np.Inf\n",
    "wait = 0\n",
    "\n",
    "history_l1_train = []\n",
    "history_l2_train = []\n",
    "history_l1_valid = []\n",
    "history_l2_valid = []\n",
    "\n",
    "\n",
    "model.addNode(20)\n",
    "#model.addLayer(20)\n",
    "start_time = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    L1_train.reset_states()\n",
    "    L2_train.reset_states()\n",
    "    for i in range(inputs_train.shape[0]):\n",
    "        input_ = inputs_train[i,:,:,:,:,:]\n",
    "        train_step(input_)\n",
    "    \n",
    "    L1_valid.reset_states()\n",
    "    L2_valid.reset_states()\n",
    "    for i in range(inputs_test.shape[0]):\n",
    "        input_ = inputs_test[i,:,:,:,:,:]\n",
    "        test_step(input_)\n",
    "    \n",
    "    \n",
    "    template = '에포크: {}, L1_train: {:.4f}, L2_train: {:.4f}, L1_vaid: {:.4f}, L2_valid: {:.4f}, 걸린 시간: {:.3f}'\n",
    "    print (template.format(epoch+1,\n",
    "                         L1_train.result(),\n",
    "                           L2_train.result(),\n",
    "                         L1_valid.result(),\n",
    "                           L2_valid.result(),\n",
    "                           (time.time() - start_time)))\n",
    "    \n",
    "    history_l1_train.append(L1_train.result())\n",
    "    history_l2_train.append(L2_train.result())\n",
    "    \n",
    "    history_l1_valid.append(L1_valid.result())\n",
    "    history_l2_valid.append(L2_valid.result())\n",
    "    \n",
    "    \n",
    "    if np.less(float(L1_valid.result() + L2_valid.result()), best):\n",
    "        best = float(L1_valid.result() + L2_valid.result())\n",
    "        best_weights = model.get_weights()\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait +=1\n",
    "        if wait >= patience:\n",
    "            model.set_weights(best_weights)\n",
    "            stopped_epoch = epoch\n",
    "            print('Early Stopped !')\n",
    "            break\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(2, figsize=(18,20))\n",
    "\n",
    "ax[0].set_title('L1')\n",
    "x = np.arange(len(history_l1_train))\n",
    "ax[0].plot(x, history_l1_train, 'r-', label = 'trainLoss')\n",
    "ax[0].plot(x, history_l1_valid, 'b-', label = 'validLoss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title('L2')\n",
    "ax[1].plot(x, history_l2_train, 'r-', label = 'trainLoss')\n",
    "ax[1].plot(x, history_l2_valid, 'b-', label = 'validLoss')\n",
    "ax[1].legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-glass",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
