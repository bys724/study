{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sudden-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as kb\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "selective-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions can be used to convert a value to a type compatible\n",
    "# with tf.train.Example.\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "designed-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image_name': tf.io.FixedLenFeature([], tf.string),\n",
    "    'target': tf.io.FixedLenFeature([], tf.string)\n",
    "}\n",
    "def _parse_image_function(example_proto):\n",
    "  # Parse the input tf.train.Example proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "def features_preprocess(features):\n",
    "    \n",
    "    # Image\n",
    "    image_raw = features['image']\n",
    "    image_ = tf.io.decode_image(image_raw)\n",
    "    image = tf.cast(image_ , tf.float32) * (1. / 255)\n",
    "    \n",
    "    # Image name\n",
    "    image_name_raw = features['image_name'].numpy().decode()\n",
    "    image_name = image_name_raw.split('_')[0]\n",
    "    image_color = image_name_raw.split('_')[1]\n",
    "    \n",
    "    # Image Label\n",
    "    target_raw = features['target'].numpy().decode()\n",
    "    target_list = target_raw.split('|')\n",
    "\n",
    "    return image, image_name, image_color, target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "manual-making",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tfrecords/train02-1363.tfrec\n"
     ]
    }
   ],
   "source": [
    "test_files = os.listdir('./test_tfrecords')\n",
    "test_files.sort()\n",
    "\n",
    "train_files = os.listdir('./train_tfrecords')\n",
    "train_files.sort()\n",
    "\n",
    "\n",
    "trial = 100\n",
    "\n",
    "for trial_ in range(trial):\n",
    "    selected = np.random.choice(len(train_files), 2, replace=False)\n",
    "\n",
    "    #inputs_single = np.empty([1, 1024, 1024, 4])\n",
    "    inputs_single = np.empty([1, 512, 512, 4])\n",
    "    outputs = np.empty([1, 1])\n",
    "    \n",
    "    for idx in selected:\n",
    "        \n",
    "\n",
    "buffer = {}\n",
    "current_file = ''\n",
    "#list_include = []\n",
    "#iist_notinclude = []\n",
    "\n",
    "\n",
    "\n",
    "for train_file in train_files[2:3]:\n",
    "    dir_ = os.path.join('train_tfrecords', train_file)\n",
    "    print(dir_)\n",
    "\n",
    "\n",
    "    raw_dataset = tf.data.TFRecordDataset(dir_)\n",
    "    \"\"\"\n",
    "    \n",
    "    for raw_record in raw_dataset:\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(raw_record.numpy())\n",
    "        print(example)\n",
    "        break\n",
    "    \"\"\"\n",
    "    parsed_dataset = raw_dataset.map(_parse_image_function)\n",
    "    for features in parsed_dataset:\n",
    "        image, image_name, image_color, target_list = features_preprocess(features)\n",
    "        \n",
    "        #if current_file is not image_name and len(buffer.keys()) is not 0:\n",
    "        #    print(current_file + ' has missing data')\n",
    "        #    break\n",
    "        #else:\n",
    "        if current_file != image_name:\n",
    "            buffer = {}\n",
    "            current_file = image_name\n",
    "            #print(current_file)\n",
    "            #print('update')\n",
    "            \n",
    "        buffer[image_color] = image\n",
    "        \n",
    "        # 하나의 이미지에 대한 4개의 색깔 이미지가 다 보인 시점\n",
    "        if len(buffer.keys()) == 4:\n",
    "            \n",
    "            if '0' in target_list:\n",
    "                \n",
    "                #if current_file not in list_include:\n",
    "                #    list_include.append(current_file)\n",
    "                    \n",
    "                input_single = tf.expand_dims(tf.concat([buffer['blue'], buffer['yellow'], buffer['green'], buffer['red'] ], 2), 0)\n",
    "                input_single = input_single[:, 256:768, 256:768, :]\n",
    "                output_ = tf.constant([[1]])\n",
    "                \n",
    "                #print(output_.shape)\n",
    "                \n",
    "                inputs_single = np.concatenate((inputs_single, input_single), axis=0)\n",
    "                outputs = np.concatenate((outputs, output_), axis=0)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                \n",
    "                #if current_file not in iist_notinclude:\n",
    "                #    iist_notinclude.append(current_file)\n",
    "                \n",
    "                input_single = tf.expand_dims(tf.concat([buffer['blue'], buffer['yellow'], buffer['green'], buffer['red'] ], 2), 0)\n",
    "                input_single = input_single[:, 256:768, 256:768, :]\n",
    "                output_ = tf.constant([[0]])\n",
    "                \n",
    "                inputs_single = np.concatenate((inputs_single, input_single), axis=0)\n",
    "                outputs = np.concatenate((outputs, output_), axis=0)\n",
    "            \n",
    "            \n",
    "        \n",
    "        #plt.imshow(image_)\n",
    "        #break\n",
    "        #image_name_ = features['image_name'].numpy()\n",
    "        #target_ = features['target'].numpy()\n",
    "\n",
    "        #display.display(display.Image(data=image_))\n",
    "        #print(image_name_)\n",
    "        #break\n",
    "inputs_single = np.delete(inputs_single, 0, axis=0)\n",
    "outputs = np.delete(outputs, 0, axis=0)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs_single, outputs)).shuffle(inputs_single.shape[0], reshuffle_each_iteration=True)\n",
    "dataset_batched = dataset.batch(16, drop_remainder=True)\n",
    "numberOfData = dataset_batched.cardinality().numpy()\n",
    "trainDataset = dataset_batched.take(int(numberOfData * 0.7))\n",
    "validDataset = dataset_batched.skip(int(numberOfData * 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "floral-voluntary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f8befc9ad68>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self, list_units):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, padding='same', activation='swish')\n",
    "        self.pool1 = MaxPooling2D()\n",
    "        \n",
    "        self.conv2 = Conv2D(32, 3, padding='same', activation='swish')\n",
    "        self.pool2 = MaxPooling2D()\n",
    "        \n",
    "        self.conv3 = Conv2D(32, 3, padding='same', activation='swish')\n",
    "        self.pool3 = MaxPooling2D()\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        self.layers_custom = self.create_layers(list_units)\n",
    "        self.lastLayer = Dense(1, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(0.001))\n",
    "        \n",
    "    def create_layers(self, list_units):\n",
    "        layers = []\n",
    "        for units in list_units:\n",
    "            layers.append(Dense(units, activation='swish', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "        return layers\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        for layer in self.layers_custom:\n",
    "            x = layer(x)\n",
    "        x = self.lastLayer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = MyModel([7, 7])\n",
    "model.load_weights('./kaggle_models/class0_77_file1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "nasty-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean()\n",
    "train_acc = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "        train_loss(loss)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_acc.update_state(labels, predictions)\n",
    "\n",
    "    \n",
    "    \n",
    "valid_loss = tf.keras.metrics.Mean()\n",
    "valid_acc = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    valid_loss(t_loss)\n",
    "    valid_acc.update_state(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "continent-offset",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-5de6cbbca5f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainDataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidDataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mexpand_dims_v2\u001b[0;34m(input, axis, name)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0mInvalidArgumentError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mout\u001b[0m \u001b[0mof\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m   \"\"\"\n\u001b[0;32m--> 435\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mexpand_dims\u001b[0;34m(input, axis, name)\u001b[0m\n\u001b[1;32m   2264\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2265\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 2266\u001b[0;31m         _ctx, \"ExpandDims\", name, input, axis)\n\u001b[0m\u001b[1;32m   2267\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2268\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "patience = 100\n",
    "stopped_epoch = 0\n",
    "best_weights = None\n",
    "best = np.Inf\n",
    "wait = 0\n",
    "\n",
    "\n",
    "history_trainLoss = []\n",
    "history_validLoss = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    clear_output(wait=True)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for images, labels in trainDataset:\n",
    "        train_step(images, tf.expand_dims(labels, axis=-1))\n",
    "    for images, labels in validDataset:\n",
    "        test_step(images, tf.expand_dims(labels, axis=-1))\n",
    "\n",
    "    template = '에포크: {}, 손실: {:.4f}, 정확도: {:.3f}, 테스트 손실: {:.4f}, 테스트 정확도: {:.3f}, 소요시간: {:.3f}'\n",
    "    print (template.format(epoch+1,\n",
    "                         train_loss.result(),\n",
    "                         train_acc.result()*100,\n",
    "                         valid_loss.result(),\n",
    "                         valid_acc.result()*100,\n",
    "                           time.time()-start_time))\n",
    "\n",
    "    history_trainLoss.append(train_loss.result())\n",
    "    history_validLoss.append(valid_loss.result())\n",
    "\n",
    "    if np.less(float(valid_loss.result()), best):\n",
    "        best = float(valid_loss.result())\n",
    "        best_weights = model.get_weights()\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait +=1\n",
    "        if wait >= patience:\n",
    "            model.set_weights(best_weights)\n",
    "            stopped_epoch = epoch\n",
    "            print('Early Stopped !')\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "fig= plt.figure(figsize=(18,20))\n",
    "plt.title('Loss')\n",
    "x = np.arange(len(history_trainLoss))\n",
    "plt.plot(x, history_trainLoss, 'r-', label = 'history_trainLoss')\n",
    "plt.plot(x, history_validLoss, 'b-', label = 'history_validLoss')\n",
    "plt.legend()\n",
    "\n",
    "model.save_weights('./kaggle_models/class0_77_file2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "deadly-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial = 100\n",
    "\n",
    "for i in range(trial):\n",
    "    np.random.choice(len(train_files), 2, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "headed-letter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "tmp1 = tf.constant(1, shape=(3, 2, 1))\n",
    "tmp2 = tf.constant(1, shape=(3, 2, 1))\n",
    "print(tf.concat([tmp1, tmp2], 2).shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "intensive-candy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(1, 1)\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "tmp1 = tf.raw_ops.Empty(shape = (1, 1), dtype=float)\n",
    "tmp2 = tf.constant([[2.0]])\n",
    "\n",
    "print(tmp1.shape)\n",
    "print(tmp2.shape)\n",
    "\n",
    "print(tf.concat([tmp1, tmp2], 0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-argentina",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
